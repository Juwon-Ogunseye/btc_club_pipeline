name: data-pipeline-workflow
on: 
  push:
  workflow_dispatch:
  schedule:
    - cron: "35 0 * * *"

jobs:
  run-data-pipeline:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repo content
        uses: actions/checkout@v4
        with:
          token: ${{ secrets.PERSONAL_ACCESS_TOKEN }}
          
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.9'
          cache: 'pip'
          
      - name: Install dependencies
        run: |
          pip install pymysql pandas duckdb python-dotenv
        
      - name: Run data pipeline
        env:
          # MotherDuck token (you already have this)
          MOTHERDUCK_TOKEN: ${{ secrets.MOTHERDUCK_TOKEN }}
          # MySQL credentials - ADD THESE TO GITHUB SECRETS
          ENDPOINT: ${{ secrets.ENDPOINT }}
          PASSWORD: ${{ secrets.PASSWORD }}
          # USER and DBNAME are hardcoded in script
        run: python main.py
        
      - name: Check for changes
        id: git-check
        run: |
          git config user.name 'github-actions'
          git config user.email 'github-actions@github.com'
          git add .
          git diff --staged --quiet || echo "changes=true" >> $GITHUB_ENV
          
      - name: Commit and push if changes
        if: env.changes == 'true'
        run: |
          git commit -m "Updates data index"
          git push